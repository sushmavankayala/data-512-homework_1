{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework 1 \n",
    "## Professionalism & Reproducibility\n",
    "The goal of this assignment is to construct, analyze, and publish a dataset of monthly article traffic for a select set of pages from English Wikipedia from July 1, 2015 through September 30, 2024. The purpose of the assignment is to develop and follow best practices for open scientific research as exemplified by your repository.\n",
    "\n",
    "For reproducing my analysis, make sure to run all the cells in order.\n",
    "\n",
    "\n",
    "### License\n",
    "Snippets of the below code were taken from a code example developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.3 - August 16, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Aquisition\n",
    "\n",
    "In order to measure article traffic from 2015-2024, we need to collect data from the Wikimedia Analytics API. The Pageviews API call ([documentation](https://doc.wikimedia.org/generated-data-platform/aqs/analytics-api/reference/page-views.html)) provides access to desktop, mobile web, and mobile app traffic data starting from July 2015 through the previous complete month.\n",
    "\n",
    "For this homework, we fetch details of all articles listed in the [provided CSV](https://drive.google.com/file/d/15_FiKhBgXB2Ch9c0gAGYzKjF0DBhEPlY/view) by calling the PageViews API with the article names. The fetched data is stored as JSON files with keys as the article title and values as the corresponding data retrieved from the API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "import os\n",
    "\n",
    "# The below module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code relies on some constants that help make the code a bit more readable. Some values might need to be updated to reflect correct paths/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# Local path to the CSV File containing the wikipedia articles of interest. Update this to point to the correct path.\n",
    "WIKIPEDIA_ARTICLES= '/Users/sushmavankayala/Documents/DATA_512/week1/rare-disease_cleaned.AUG.2024.csv'\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# Adding a small delay to each request to the Pageviews API, to not exceed 100 requests per second\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# Required headers for calling the Wikimedia API.\n",
    "# They expect an email address for them to contact in case of issues(rate limiting, etc)\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<vanksu@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024',\n",
    "}\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_DESKTOP = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",   # Start date and end date as provided in the HW instructions\n",
    "    \"end\":         \"2024093000\"\n",
    "}\n",
    "\n",
    "# Creating templates for Mobile App and Mobile web by cloning the desktop template and updating the \"access\" value.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_MOBILE_APP = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_DESKTOP.copy()\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_MOBILE_APP[\"access\"] = \"mobile-app\"\n",
    "\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_MOBILE_WEB = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_DESKTOP.copy()\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_MOBILE_WEB[\"access\"] = \"mobile-web\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the analysis requires writing to files and reading from files, I decided to create some variables and functions to maintain consistency among different steps and streamline the file access across this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filenames using start and end dates in the format YYYYMM\n",
    "# These filenames will be used for dumping the acquired data, and for reading the data for analysis\n",
    "start_date = \"201507\"\n",
    "end_date = \"202409\"\n",
    "\n",
    "# Filename for Desktop views\n",
    "desktop_views_file = f\"rare-disease_monthly_desktop_{start_date}-{end_date}.json\"\n",
    "\n",
    "# Filename for Mobile views\n",
    "mobile_views_file = f\"rare-disease_monthly_mobile_{start_date}-{end_date}.json\"\n",
    "\n",
    "# Filename for Cumulative views\n",
    "cumulative_views_file = f\"rare-disease_monthly_cumulative_{start_date}-{end_date}.json\"\n",
    "\n",
    "def create_directory(target_path):\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(target_path):\n",
    "        try:\n",
    "            # Attempt to create the directory\n",
    "            os.makedirs(target_path)\n",
    "        except Exception as e:\n",
    "            print(\"Unable to create path\" + target_path)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    Procedures/Functions for Data Aquisition\n",
    "#\n",
    "\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_DESKTOP,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "\n",
    "    \"\"\"\n",
    "    Makes a REST API call to the PageViews endpoint and returns the JSON response.\n",
    "\n",
    "    Args:\n",
    "        article_title (str): The title of the article to request pageviews for.\n",
    "        endpoint_url (str): The URL of the API endpoint.\n",
    "        endpoint_params (str): The parameters for the API request.\n",
    "        request_template (dict): The template for the request parameters.\n",
    "        headers (dict): The headers for the API request.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The JSON response from the API call, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'), safe='')\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "\n",
    "def write_to_file(json_data, filename, mode = \"a+\"):\n",
    "    \"\"\"\n",
    "    Writes data to a specified file in JSON format.\n",
    "\n",
    "    Args:\n",
    "        json_data (any): The data to be written to the file. \n",
    "        filename (str): The path to the file where the data will be written.\n",
    "        mode (str, optional): The file mode in which to open the file. \n",
    "                              Defaults to \"a+\" (append and read). Other common modes include:\n",
    "                              - \"w\": write (overwrites existing content)\n",
    "                              - \"r\": read (file must exist)\n",
    "    \n",
    "    Returns:\n",
    "        None: This function does not return a value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, mode) as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal is to produce three files as follows:\n",
    "* Monthly mobile access - The API separates mobile access types into two separate requests, we will need to sum these to make one count for all mobile pageviews. Store the mobile access data in a file called:\n",
    "rare-disease_monthly_mobile_\\<startYYYYMM>-\\<endYYYYMM>.json\n",
    "* Monthly desktop access - Monthly desktop page traffic is based on one single request. We should store the desktop access data in a file called:\n",
    "rare-disease_monthly_desktop_\\<startYYYYMM>-\\<endYYYYMM>.json\n",
    "* Monthly cumulative - Monthly cumulative data is the sum of all mobile, and all desktop traffic per article. We should store the monthly cumulative data in a file called:\n",
    "rare-disease_monthly_cumulative_\\<startYYYYMM>-\\<endYYYYMM>.json\n",
    "For all of the files the \\<startYYYYMM> and \\<endYYYYMM> represent the starting and ending year and month as integer text strings.\n",
    "\n",
    "To understand the progress of data aquisition, print statements have been added to the below code.\n",
    "\n",
    "Note: This step takes ~40mins to using a laptop with Apple M1 Pro chip and having 16GB RAM.\n",
    "Suggestion: The generated json dumps are available in the git repository under the folder named generated_files. You can skip the next 2 cells, and proceed to the Data Analysis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of wikipedia articles from the CSV file and create a dataframe\n",
    "wiki_list = pd.read_csv(WIKIPEDIA_ARTICLES)\n",
    "\n",
    "# Order the list by article title, i.e, disease name in the CSV\n",
    "wiki_list = wiki_list.sort_values(\"disease\").reset_index(drop=True)\n",
    "\n",
    "# Create empty dictionaries for each file to be created\n",
    "desktop_views = {}\n",
    "mobile_views = {}\n",
    "cumulative_views = {}\n",
    "\n",
    "for index, row in wiki_list.iterrows():\n",
    "\n",
    "    # Fetch article title from dataframe\n",
    "    article_title = row[\"disease\"]\n",
    "    print(\"Getting pageview data for disease:\", article_title, \", index:\", index)\n",
    "\n",
    "    # Fetch data on desktop views for the article\n",
    "    desktop_views_per_title = request_pageviews_per_article(article_title, request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_DESKTOP)\n",
    "    desktop_views_per_title = desktop_views_per_title[\"items\"]\n",
    "    # Convert JSON to dataframe\n",
    "    desktop_views_df = pd.DataFrame(desktop_views_per_title)\n",
    "    # Drop \"access\" column\n",
    "    desktop_views_df = desktop_views_df.drop('access', axis=1)\n",
    "    # Convert dataframe back to JSON Array, and add it to the desktop_views dictionary with article title as the key\n",
    "    desktop_views[article_title] = json.loads(desktop_views_df.to_json(orient = \"records\"))\n",
    "    print(\"Desktop views added to dictionary\")\n",
    "\n",
    "    \n",
    "    # Fetching data on Mobile views for the article consists of 2 parts\n",
    "    \n",
    "    # Fetch data on Mobile App views for the article\n",
    "    mobile_app_views_per_title = request_pageviews_per_article(article_title, request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_MOBILE_APP)\n",
    "    mobile_app_views_per_title = mobile_app_views_per_title[\"items\"]\n",
    "    # Convert JSON to dataframe\n",
    "    mobile_app_views_df = pd.DataFrame(mobile_app_views_per_title)\n",
    "\n",
    "    # Fetch data on Mobile Web views for the article\n",
    "    mobile_web_views_per_title = request_pageviews_per_article(article_title, request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE_MOBILE_WEB)\n",
    "    mobile_web_views_per_title = mobile_web_views_per_title[\"items\"]\n",
    "    # Convert JSON to dataframe\n",
    "    mobile_web_views_df = pd.DataFrame(mobile_web_views_per_title)\n",
    "\n",
    "    # Merge the Mobile App and Mobile web based on the article name and timestamp. \n",
    "    # Except views and access, all other columns should have identical values. \n",
    "    # To avoid duplicate column creation on join, add the other fields in the join condition\n",
    "    mobile_views_df = pd.merge(mobile_app_views_df, mobile_web_views_df, on=[\"project\", \"article\", \"granularity\",\"timestamp\", \"agent\"], how=\"outer\")\n",
    "    # Total mobile views is calculated by adding the app and web mobile views.\n",
    "    mobile_views_df[\"views\"] = mobile_views_df[\"views_x\"] + mobile_views_df[\"views_y\"]\n",
    "    # Drop the access and views columns specific to mobile app and mobile web views dataframes.\n",
    "    mobile_views_df = mobile_views_df.drop(['access_x','access_y', \"views_x\", \"views_y\"], axis=1)\n",
    "    # Convert dataframe back to JSON Array, and add it to the mobile_views dictionary with article title as the key\n",
    "    mobile_views[article_title] = json.loads(mobile_views_df.to_json(orient = \"records\"))\n",
    "    print(\"Mobile views added to dictionary\")\n",
    "\n",
    "    # For calculating cumulative views of article, we need to sum each articles monthly views for desktop and mobile.\n",
    "    # To facilitate this, merge the dataframes containing desktop views and mobile views based on the article name and timestamp\n",
    "    # Except views, all other columns should have identical values.\n",
    "    # To avoid duplicate column creation on join, add the other fields in the join condition\n",
    "    cumulative_views_df = pd.merge(desktop_views_df, mobile_views_df, on=[\"project\", \"article\", \"granularity\",\"timestamp\", \"agent\"], how=\"outer\")\n",
    "    # Cumulative views is calculated by adding the desktop and mobile views.\n",
    "    cumulative_views_df[\"views\"] = cumulative_views_df[\"views_x\"] + cumulative_views_df[\"views_y\"]\n",
    "    # Drop the views columns specific to desktop and mobile views dataframes.\n",
    "    cumulative_views_df = cumulative_views_df.drop([\"views_x\", \"views_y\"], axis=1)\n",
    "    # Convert dataframe back to JSON Array, and add it to the cumulative_views dictionary with article title as the key\n",
    "    cumulative_views[article_title] = json.loads(cumulative_views_df.to_json(orient = \"records\"))\n",
    "    print(\"Cumulative views added to dictionary\")\n",
    "\n",
    "    print(\"--------\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we have a handle on 3 dictionaries, each having information about the corresponding monthly views per article title. \n",
    "Next step is to dump these dictionaries into JSON files for easier access in future. \n",
    "The data in these JSON files will be used to run analysis on the view counts per access type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the dictionary containing views into the corresponding JSON file\n",
    "\n",
    "# Create the target directory\n",
    "target_folder = \"generated_files\"\n",
    "create_directory(target_folder)\n",
    "\n",
    "# Dumping Desktop views into file\n",
    "write_to_file(desktop_views, filename=f\"{target_folder}/{desktop_views_file}\", mode=\"w+\")\n",
    "\n",
    "# Dumping Mobile views into file\n",
    "write_to_file(mobile_views, filename=f\"{target_folder}/{mobile_views_file}\", mode=\"w+\")\n",
    "\n",
    "# Dumping Cumulative views into file\n",
    "write_to_file(cumulative_views, filename=f\"{target_folder}/{cumulative_views_file}\", mode=\"w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Analysis\n",
    "\n",
    "We want to do a basic visual analysis of the data by producing 3 different graphs.\n",
    "\n",
    "* Maximum Average and Minimum Average - Contains time series for the articles that have the highest average page requests and the lowest average page requests for desktop access and mobile access over the entire time series. \n",
    "* Top 10 Peak Page Views - Contains time series for the top 10 article pages by largest (peak) page views over the entire time series by access type. \n",
    "* Fewest Months of Data - Should show the 10 articles with the fewest months of data for desktop access and the 10 articles with the fewest months of data for mobile access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Processing\n",
    "\n",
    "Considering the data required to create these graphs, I have decided to write a helper function that helps to aggregate the data corresonding to a particular article and stores information like max views, min views, average views, and count of months for which the data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    Procedures/Functions for Data Processing\n",
    "#\n",
    "\n",
    "def aggregate_article_data(filename):\n",
    "    \"\"\"\n",
    "    Aggregates view data from a JSON file containing data for multiple articles.\n",
    "\n",
    "    Args:\n",
    "        filename: The path to the JSON file containing article view data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing aggregated view statistics \n",
    "              for an article, including counts, maximum, minimum, and average views.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to hold aggregated data for each article\n",
    "    aggregated_counts = []\n",
    "\n",
    "    # Open the specified JSON file for reading\n",
    "    with open(filename) as json_file:\n",
    "        views = json.load(json_file)\n",
    "        # Iterate over each article's title and its associated view data\n",
    "        for article_title, view_data in views.items():\n",
    "            \n",
    "            # Convert the view data for the article into a pandas DataFrame\n",
    "            views_df = pd.DataFrame(view_data)\n",
    "            max_views = views_df['views'].max()\n",
    "            min_views= views_df['views'].min()\n",
    "\n",
    "            # Prepare a dictionary to hold aggregated statistics for the article\n",
    "            aggregate = {\n",
    "                \"article_title\": article_title,  # Title of the article\n",
    "                \"count\": views_df.shape[0],      # Total number of data entries for the article\n",
    "                \"max_views\": max_views,           # Maximum views recorded\n",
    "                \"max_month\": views_df[views_df['views'] == max_views].iloc[0]['timestamp'],  # Timestamp of max views\n",
    "                \"min_views\": min_views,           # Minimum views recorded\n",
    "                \"min_month\": views_df[views_df['views'] == min_views].iloc[0]['timestamp'],  # Timestamp of min views\n",
    "                \"avg_views\": views_df['views'].mean(),  # Average views computed\n",
    "                \"data\": view_data                 # Raw view data for the article\n",
    "            }\n",
    "\n",
    "            # Append the aggregated data for the current article to the list\n",
    "            aggregated_counts.append(aggregate)\n",
    "\n",
    "    return aggregated_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggegate data from the JSON files created by the end of Data Aquisition steps\n",
    "target_folder = \"generated_files\"\n",
    "# Aggregate JSON file containing desktop views data\n",
    "desktop_aggregate = aggregate_article_data(f\"{target_folder}/{desktop_views_file}\")\n",
    "desktop_aggregate_df = pd.DataFrame(desktop_aggregate)\n",
    "\n",
    "# Aggregate JSON file containing mobile views data\n",
    "mobile_aggregate = aggregate_article_data(f\"{target_folder}/{mobile_views_file}\")\n",
    "mobile_aggregate_df = pd.DataFrame(mobile_aggregate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Visualization\n",
    "\n",
    "Now that we have the aggregated data (both mobile and desktop), we proceed with plotting the above mentioned graphs.\n",
    "\n",
    "For a clear segregation between lines representing Desktop views and Mobile views, I've decided to use solid lines for desktop, and dashed lines for mobile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package for drawing plots.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directory for saving the generated plots\n",
    "target_folder_plots = \"generated_plots\"\n",
    "create_directory(target_folder_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Graph 1: Maximum Average and Minimum Average\n",
    "This graph contains time series for the articles that have the highest average page requests and the lowest average page requests for desktop access and mobile access over the entire time series. The graph has four lines (max desktop, min desktop, max mobile, min mobile)\n",
    "\n",
    "Helper functions are created to find the articles with max average views and min average views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_with_max_average(aggregated_df):\n",
    "    \"\"\"\n",
    "    Retrieves the article with the highest average views from the aggregated DataFrame.\n",
    "\n",
    "    Args:\n",
    "        aggregated_df (pd.DataFrame): A DataFrame containing aggregated statistics \n",
    "                                       for articles, including average views.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series representing the row of the article with the maximum \n",
    "                    average views.\n",
    "    \"\"\"\n",
    "    # Use idxmax to find the index of the article with the highest average views\n",
    "    # and return the corresponding row from the DataFrame\n",
    "    return aggregated_df.iloc[aggregated_df[\"avg_views\"].idxmax()]\n",
    "\n",
    "def article_with_min_average(aggregated_df):\n",
    "    \"\"\"\n",
    "    Retrieves the article with the least average views from the aggregated DataFrame.\n",
    "\n",
    "    Args:\n",
    "        aggregated_df (pd.DataFrame): A DataFrame containing aggregated statistics \n",
    "                                       for articles, including average views.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series representing the row of the article with the minimum \n",
    "                    average views.\n",
    "    \"\"\"\n",
    "    # Use idxmin to find the index of the article with the highest average views\n",
    "    # and return the corresponding row from the DataFrame\n",
    "    return aggregated_df.iloc[aggregated_df[\"avg_views\"].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding details of articles with highest and least avg views.\n",
    "\n",
    "# Initialize an empty list to store time series data for articles\n",
    "time_series_data = []\n",
    "\n",
    "# Find article for each condition given. For each article found, \n",
    "# Convert the article's view data into a DataFrame\n",
    "# Convert the 'timestamp' column to datetime format for easier time series manipulation\n",
    "\n",
    "# Retrieve the article with the highest average views from the desktop aggregated DataFrame\n",
    "article_max_d = article_with_max_average(desktop_aggregate_df)\n",
    "print(\"Article with highest average views(desktop):\", article_max_d[\"article_title\"])\n",
    "article_max_d_df = pd.DataFrame(article_max_d[\"data\"])\n",
    "article_max_d_df['date'] = pd.to_datetime(article_max_d_df['timestamp'], format='%Y%m%d%H')\n",
    "\n",
    "# Retrieve the article with the lowest average views from the desktop aggregated DataFrame\n",
    "article_min_d = article_with_min_average(desktop_aggregate_df)\n",
    "print(\"Article with lowest average views(desktop):\", article_min_d[\"article_title\"])\n",
    "article_min_d_df = pd.DataFrame(article_min_d[\"data\"])\n",
    "article_min_d_df['date'] = pd.to_datetime(article_min_d_df['timestamp'], format='%Y%m%d%H')\n",
    "\n",
    "# Retrieve the article with the highest average views from the mobile aggregated DataFrame\n",
    "article_max_m = article_with_max_average(mobile_aggregate_df)\n",
    "print(\"Article with Highest average views(mobile):\", article_max_m[\"article_title\"])\n",
    "article_max_m_df = pd.DataFrame(article_max_m[\"data\"])\n",
    "article_max_m_df['date'] = pd.to_datetime(article_max_m_df['timestamp'], format='%Y%m%d%H')\n",
    "\n",
    "# Retrieve the article with the lowest average views from the mobile aggregated DataFrame\n",
    "article_min_m = article_with_min_average(mobile_aggregate_df)\n",
    "print(\"Article with least average views(mobile):\", article_min_m[\"article_title\"])\n",
    "article_min_m_df = pd.DataFrame(article_min_m[\"data\"])\n",
    "article_min_m_df['date'] = pd.to_datetime(article_min_m_df['timestamp'], format='%Y%m%d%H')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Plot a seperate line for each case\n",
    "plt.plot(article_max_d_df[\"date\"], article_max_d_df[\"views\"], label=\"Max Avg(Desktop):\" + article_max_d[\"article_title\"] )\n",
    "plt.plot(article_min_d_df[\"date\"], article_min_d_df[\"views\"], label=\"Min Avg(Desktop):\" + article_min_d[\"article_title\"])\n",
    "plt.plot(article_max_m_df[\"date\"], article_max_m_df[\"views\"], label=\"Max Avg(Mobile):\" + article_max_m[\"article_title\"], linestyle=\"dashed\")\n",
    "plt.plot(article_min_m_df[\"date\"], article_min_m_df[\"views\"], label=\"Min Avg(Mobile):\" + article_min_m[\"article_title\"], linestyle=\"dashed\")\n",
    "\n",
    "# Set the X-axis and Y axis labels, along with the title of the plot.\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Views')\n",
    "plt.title('Maximum Average and Minimum Average - Desktop, Mobile')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Save plot as png\n",
    "plt.savefig(\"generated_plots/max_avg_min_avg.png\", bbox_inches='tight') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Analysis\n",
    "From the above graph, we see that the article on disease \"Black Death\" has the highest average page requests, both in deskop and mobile. We see that the number of views peaked during the earlier months of the year 2020. We also see that in the month with highest views, the number of times the article was viewed on mobile is more than twice the number of views via desktop.\n",
    "\n",
    "The article on disease \"Filippi Syndrome\" has the least average page requests, both in desktop and mobile. We see that the data on views is available only from the later months of year 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Graph 2: Top 10 Peak Page Views\n",
    "This graph contains time series for the top 10 article pages by largest (peak) page views over the entire time series by access type. To find the top 10, first find the month for each article that contains the highest (peak) page views, and then order the articles by these peak values. The graph should contain the top 10 for desktop and top 10 for mobile access (20 lines).\n",
    "\n",
    "Helper function is created to find the articles with max average views and min average views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_peaks(aggregated_df):\n",
    "    \"\"\"\n",
    "    Retrieves the top 10 articles with the largest max views from the aggregated DataFrame.\n",
    "\n",
    "    Args:\n",
    "        aggregated_df (pd.DataFrame): A DataFrame containing aggregated statistics \n",
    "                                       for articles, including average views.\n",
    "\n",
    "    Returns:\n",
    "        pd.dataframe: A dataframe containing 10 rows having maximum monthly views, where each row is an article.\n",
    "    \"\"\"\n",
    "    # sort the data frame based on max_views in decending order.\n",
    "    # The top 10 rows of the sorted dataframe represent the articles with highest peaks.\n",
    "    top_10 =  aggregated_df.sort_values([\"max_views\", \"article_title\"], ascending=False).head(10)\n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Process and plot top 10 peaks for desktop data\n",
    "top_10_peaks_desktop_df = top_10_peaks(desktop_aggregate_df)\n",
    "for index, row in top_10_peaks_desktop_df.iterrows():\n",
    "    # Create temporary dataframe for each peak\n",
    "    temp_df = pd.DataFrame(row[\"data\"])\n",
    "    # Convert timestamp to datetime\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['timestamp'], format='%Y%m%d%H')\n",
    "    # Plot desktop data\n",
    "    plt.plot(temp_df[\"date\"], temp_df[\"views\"], label=\"Desktop:\" + row[\"article_title\"] + \"[Peak:\"+str(row[\"max_views\"]) + \"]\")\n",
    "\n",
    "# Process and plot top 10 peaks for mobile data\n",
    "top_10_peaks_mobile_df = top_10_peaks(mobile_aggregate_df)\n",
    "for index, row in top_10_peaks_mobile_df.iterrows():\n",
    "    # Create temporary dataframe for each peak\n",
    "    temp_df = pd.DataFrame(row[\"data\"])\n",
    "    # Convert timestamp to datetime\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['timestamp'], format='%Y%m%d%H')\n",
    "    # Plot mobile data with dashed lines\n",
    "    plt.plot(temp_df[\"date\"], temp_df[\"views\"], label=\"Mobile:\" + row[\"article_title\"]+ \"[Peak:\"+str(row[\"max_views\"]) + \"]\", linestyle='dashed')\n",
    "\n",
    "# Set the X-axis and Y axis labels, along with the title of the plot.\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Views')\n",
    "plt.title('Top 10 Peak Page Views - Desktop, Mobile')\n",
    "\n",
    "# Add legend to distinguish between different lines\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save plot as png\n",
    "plt.savefig(\"generated_plots/top_10_peak_page_views.png\", bbox_inches='tight') \n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Analysis\n",
    "From the above graph, we see that majority of the evident peaks correspond to article views on mobile(represented by dashed lines). The legend beside the graph provides details about the article names, the access type (Desktop / Mobile) and the number of times it was viewed in the articles \"most popular\" month.\n",
    "\n",
    "We see that the articles titled \"Pandemic\" and \"Black Death\" are the top 2 in both desktop and mobile. While \"Pandemic\" has the largest desktop views, \"Black Death\" has the highest number of mobile views in a given month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Graph 3: Fewest Months of Data \n",
    "This graph shows pages that have the fewest months of available data. It shows the 10 articles with the fewest months of data for desktop access and the 10 articles with the fewest months of data for mobile access.\n",
    "\n",
    "A helper function is created to find the articles with max average views and min average views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewest_months(aggregated_df):\n",
    "    \"\"\"\n",
    "    Retrieves 10 articles with the fewest months of data from the aggregated DataFrame.\n",
    "\n",
    "    Args:\n",
    "        aggregated_df (pd.DataFrame): A DataFrame containing aggregated statistics \n",
    "                                       for articles, including average views.\n",
    "\n",
    "    Returns:\n",
    "        pd.dataframe: A dataframe containing 10 rows having fewest months of data, where each row is an article.\n",
    "    \"\"\"\n",
    "    fewest_data = aggregated_df.sort_values([\"count\", \"article_title\"]).head(10)\n",
    "    return fewest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Process and plot 10 articles with fewest data for desktop data\n",
    "fewest_months_desktop_df = fewest_months(desktop_aggregate_df)\n",
    "for index, row in fewest_months_desktop_df.iterrows():\n",
    "    # Create temporary dataframe for each article\n",
    "    temp_df = pd.DataFrame(row[\"data\"])\n",
    "    # Convert timestamp to datetime\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['timestamp'], format='%Y%m%d%H')\n",
    "    # Plot desktop data\n",
    "    plt.plot(temp_df[\"date\"], temp_df[\"views\"], label=\"Desktop:\" + row[\"article_title\"] )\n",
    "\n",
    "# Process and plot 10 articles with fewest data for mobile data\n",
    "fewest_months_mobile_df = fewest_months(mobile_aggregate_df)\n",
    "for index, row in fewest_months_mobile_df.iterrows():\n",
    "    # Create temporary dataframe for each article\n",
    "    temp_df = pd.DataFrame(row[\"data\"])\n",
    "    # Convert timestamp to datetime\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['timestamp'], format='%Y%m%d%H')\n",
    "    # Plot mobile data with dashed lines\n",
    "    plt.plot(temp_df[\"date\"], temp_df[\"views\"], label=\"Mobile:\" + row[\"article_title\"], linestyle='dashed' )\n",
    "\n",
    "# Set the X-axis and Y axis labels, along with the title of the plot.\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Views')\n",
    "plt.title('Fewest Months of Data - Desktop, Mobile')\n",
    "\n",
    "# Add legend to distinguish between different lines\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Save plot as png\n",
    "plt.savefig(\"generated_plots/fewest_months_of_data.png\", bbox_inches='tight') \n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Analysis\n",
    "From the above graph, we see that the articles having data for the least amount of months are having a fairly low number of views; with an exception of articles on \"COVID-19 vaccine misinformation and hesitancy\", and \"Hemolytic jaundice\". Even then, the view counts in this graph are fairly small in comparision to the previous graphs where views are in the order of 10^6.\n",
    "\n",
    "Interestingly, we see that the view trend for the article on COVID-19 vaccine misinformation is fairly distinct between the access types desktop and mobile; especially in the earlier months of 2023. We might need more domain knowledge to find out if there is a specific reason for this behaviour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
